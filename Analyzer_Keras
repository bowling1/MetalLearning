import matplotlib.pyplot as plt
import numpy as np
import scipy.io.wavfile
import librosa
import librosa.display
from scipy import misc
from PIL import Image
import pickle 

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization
from keras.optimizers import Adam
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import LearningRateScheduler

# ------------------------------------------------------------------------------------------------------------------------------------
# Turns an image in a folder into a 2d numpy array. In this case, it also inverts the color and normalizes it from (0, 1)
def image_to_numpy_array(img_location):
	img = (misc.imread(img_location, flatten=True))/255
	img = img.tolist()
	for i in range(49,126):
	    for n in range(len(img[0])):
	        img[i][n] = [img[i][n]]
	np.array(img)
	return img[50:125]


Cavern_sections = []
Doom_sections = []
Test_sections = []


for i in range(166):
	Cavern_sections.append([1, image_to_numpy_array('TempImage\\Cavern_Death0_' + str(i) + '.jpg')])
	print(i)

for i in range(185):
	Doom_sections.append([0, image_to_numpy_array('TempImage\\Classic_Doom0_' + str(i) + '.jpg')])
	print(i)

for i in range(156):
	Cavern_sections.append([1, image_to_numpy_array('TempImage\\Cavern_Death1_' + str(i) + '.jpg')])
	print(i)

for i in range(205):
	Doom_sections.append([0, image_to_numpy_array('TempImage\\Classic_Doom1_' + str(i) + '.jpg')])
	print(i)

for i in range(206):
	Cavern_sections.append([1, image_to_numpy_array('TempImage\\Cavern_Death2_' + str(i) + '.jpg')])
	print(i)

for i in range(225):
	Doom_sections.append([0, image_to_numpy_array('TempImage\\Classic_Doom2_' + str(i) + '.jpg')])
	print(i)

for i in range(105):
	Cavern_sections.append([1, image_to_numpy_array('TempImage\\Cavern_Death3_' + str(i) + '.jpg')])
	print(i)

for i in range(209):
	Doom_sections.append([0, image_to_numpy_array('TempImage\\Classic_Doom3_' + str(i) + '.jpg')])
	print(i)

for i in range(132):
	Cavern_sections.append([1, image_to_numpy_array('TempImage\\Cavern_Death4_' + str(i) + '.jpg')])
	print(i)

for i in range(116):
	Doom_sections.append([0, image_to_numpy_array('TempImage\\Classic_Doom4_' + str(i) + '.jpg')])
	print(i)

for i in range(106):
	Test_sections.append([1, image_to_numpy_array('TempImage\\Cavern_Death5_' + str(i) + '.jpg')])
	print(i)

for i in range(119):
	Test_sections.append([0, image_to_numpy_array('TempImage\\Classic_Doom5_' + str(i) + '.jpg')])
	print(i)

for i in range(173):
	Cavern_sections.append([1, image_to_numpy_array('TempImage\\Cavern_Death6_' + str(i) + '.jpg')])
	print(i)

for i in range(126):
	Doom_sections.append([0, image_to_numpy_array('TempImage\\Classic_Doom6_' + str(i) + '.jpg')])
	print(i)

# ----------------------------------------

full_data = Cavern_sections + Doom_sections
pre_x = []
pre_y = []
test_x = []
test_y = []
pickle.dump(full_data, open("spectro_gallery.sav", 'wb'))
for i in range(len(full_data)):
	pre_x.append(full_data[i][1])
	pre_y.append(full_data[i][0])

for i in range(len(Test_sections)):
	test_x.append(Test_sections[i][1])
	test_y.append(Test_sections[i][0])

#x_train, x_val, y_train, y_val = train_test_split(pre_x, pre_y, test_size=0.2)
#x_train = np.array(x_train)
#x_val = np.array(x_val)
#x_train = x_train.reshape(-1, 480, 640, 1)
#x_val = x_val.reshape(-1, 480, 640, 1)
pre_x = np.array(pre_x)
test_x = np.array(test_x)

model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=(75, 192, 1), activation='relu', ))
model.add(MaxPool2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPool2D((2, 2)))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPool2D((2, 2)))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPool2D((2, 2)))
model.add(Flatten())
model.add(Dropout(0.5))
model.add(Dense(512, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

datagen = ImageDataGenerator()
model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])
annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 **x)   #optional (adjusts learning rate over time)
hist = model.fit_generator(datagen.flow(pre_x, pre_y, batch_size=8), steps_per_epoch=500, epochs=10, verbose=2, validation_data=(test_x, test_y))
final_loss, final_acc = model.evaluate(test_x, test_y, verbose=0)

print(final_acc)
print(final_loss)

y_hat = model.predict(test_x)     # This is a list of lists, where each sublist represents an image, and each number in it represents the likelihood of it being the associated number
y_pred = []
for i in range(len(y_hat)):
    y_pred.append(round(y_hat[i][0]))
cm = confusion_matrix(test_y, y_pred)
print(cm)

